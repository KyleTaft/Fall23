{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FROM LESSON 12 and 13 ####\n",
    "\n",
    "# Define descent methods\n",
    "def gradient_method_backtracking(f, g, x0, s=2, alpha=1/4, beta=0.5, epsilon=1e-5):\n",
    "    x = x0\n",
    "    grad = g(x)\n",
    "    fun_val = f(x)\n",
    "    ite = 0\n",
    "    while (np.linalg.norm(grad) > epsilon) & (ite < 1e4):\n",
    "        ite = ite + 1\n",
    "        t = s\n",
    "        while fun_val - f(x - t*grad) <= alpha * t * np.linalg.norm(grad)**2:\n",
    "            t = beta*t\n",
    "            if t < 1e-10:\n",
    "                break\n",
    "\n",
    "            \n",
    "        x = x - t*grad\n",
    "        grad = g(x)\n",
    "        fun_val = f(x)\n",
    "    return x, fun_val, ite\n",
    "\n",
    "def damped_newton(f, g, h, x0, s=1,alpha=1/2, beta=1/2, epsilon=1e-8):\n",
    "    x = x0\n",
    "    x_all = [x0]\n",
    "    grad = g(x)\n",
    "    hess = h(x)\n",
    "    ite = 0\n",
    "    while (np.linalg.norm(grad) > epsilon) & (ite < 1e4):\n",
    "        ite = ite + 1\n",
    "        d =  np.linalg.solve(hess, grad)\n",
    "        t = s\n",
    "        while f(x) - f(x - t*d) < alpha * t * d.T@grad:\n",
    "            t = beta*t\n",
    "        x = x - t*d\n",
    "#        print(t)\n",
    "        x_all.append(x)\n",
    "        grad = g(x)\n",
    "        hess = h(x)\n",
    "        #print(\"iter_number = %3d fun_val = %2.10f\" %(ite, f(x)))\n",
    "        if ite == 1e2:\n",
    "            break\n",
    "        if iter == 1e4:\n",
    "            print(\"do not converge!!!\")\n",
    "    return x, x_all, ite\n",
    "\n",
    "def hybrid_newton(f, g, h, x0, s=1,alpha=1/2, beta=1/2, epsilon=1e-8):\n",
    "    x = x0\n",
    "    x_all = [x0]\n",
    "    grad = g(x)\n",
    "    hess = h(x)\n",
    "    ite = 0\n",
    "    while (np.linalg.norm(grad) > epsilon) & (ite < 1e4):\n",
    "        ite = ite + 1\n",
    "        d =  np.linalg.solve(hess, grad)\n",
    "        t = s\n",
    "        while f(x) - f(x - t*d) < alpha * t * d.T@grad:\n",
    "            t = beta*t\n",
    "        x = x - t*d\n",
    "#        print(t)\n",
    "        x_all.append(x)\n",
    "        grad = g(x)\n",
    "        hess = h(x)\n",
    "        #print(\"iter_number = %3d fun_val = %2.10f\" %(ite, f(x)))\n",
    "        if ite == 1e2:\n",
    "            break\n",
    "        if iter == 1e4:\n",
    "            print(\"do not converge!!!\")\n",
    "    return x, x_all, ite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions and parameters\n",
    "f = lambda x: 0.25*x[0,0]**4 + 0.25*x[0,1]**4 - x[0,0]*x[0,1] + 4\n",
    "g = lambda x: np.array([x[0,0]**3 - x[0,1], x[0,1]**3 - x[0,0]])\n",
    "h = lambda x: np.array([[3*x[0,0]**2, -1], [-1, 3*x[0,1]**2]])\n",
    "\n",
    "init_1 = np.array([[4, 4]])\n",
    "init_2 = np.array([[0.25, 0.25]])\n",
    "init_3 = np.array([[50, 10]])\n",
    "init_4 = np.array([[30, 30]])\n",
    "\n",
    "s = 1\n",
    "alpha = 1/2\n",
    "beta = 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial point:  [[4 4]]\n",
      "\n",
      "\n",
      "gradient descent\n",
      "number of iterations:  20\n",
      "final point:  [[1.00000185 1.00000185]]\n",
      "final function value:  3.5000000000068425\n",
      "\n",
      "\n",
      "damped newton\n",
      "number of iterations:  10\n",
      "final point:  [[1. 1.]]\n",
      "final function value:  3.5\n",
      "\n",
      "\n",
      "hybrid newton\n",
      "number of iterations:  10\n",
      "final point:  [[1. 1.]]\n",
      "final function value:  3.5\n",
      "\n",
      "\n",
      "initial point:  [[0.25 0.25]]\n",
      "\n",
      "\n",
      "gradient descent\n",
      "number of iterations:  5\n",
      "final point:  [[0.99999733 0.99999733]]\n",
      "final function value:  3.500000000014232\n",
      "\n",
      "\n",
      "damped newton\n",
      "number of iterations:  3\n",
      "final point:  [[-2.98644667e-12 -2.98644667e-12]]\n",
      "final function value:  4.0\n",
      "\n",
      "\n",
      "hybrid newton\n",
      "number of iterations:  3\n",
      "final point:  [[-2.98644667e-12 -2.98644667e-12]]\n",
      "final function value:  4.0\n",
      "\n",
      "\n",
      "initial point:  [[50 10]]\n",
      "\n",
      "\n",
      "gradient descent\n",
      "number of iterations:  27\n",
      "final point:  [[1. 1.]]\n",
      "final function value:  3.5\n",
      "\n",
      "\n",
      "damped newton\n",
      "number of iterations:  14\n",
      "final point:  [[1. 1.]]\n",
      "final function value:  3.5\n",
      "\n",
      "\n",
      "hybrid newton\n",
      "number of iterations:  14\n",
      "final point:  [[1. 1.]]\n",
      "final function value:  3.5\n",
      "\n",
      "\n",
      "initial point:  [[30 30]]\n",
      "\n",
      "\n",
      "gradient descent\n",
      "number of iterations:  47\n",
      "final point:  [[-1. -1.]]\n",
      "final function value:  3.5\n",
      "\n",
      "\n",
      "damped newton\n",
      "number of iterations:  13\n",
      "final point:  [[1. 1.]]\n",
      "final function value:  3.5\n",
      "\n",
      "\n",
      "hybrid newton\n",
      "number of iterations:  13\n",
      "final point:  [[1. 1.]]\n",
      "final function value:  3.5\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-5eab1b5b186f>:2: RuntimeWarning: overflow encountered in long_scalars\n",
      "  f = lambda x: 0.25*x[0,0]**4 + 0.25*x[0,1]**4 - x[0,0]*x[0,1] + 4\n"
     ]
    }
   ],
   "source": [
    "# Run gradient descent\n",
    "for point in [init_1, init_2, init_3, init_4]:\n",
    "    print(\"initial point: \", point)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"gradient descent\")\n",
    "    x, fun_val, i = gradient_method_backtracking(f, g, point, s, alpha, beta)\n",
    "    print(\"number of iterations: \", i)\n",
    "    print(\"final point: \", x)\n",
    "    print(\"final function value: \", fun_val)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"damped newton\")\n",
    "    x, x_all, i = damped_newton(f, g, h, point, s, alpha, beta)\n",
    "    print(\"number of iterations: \", i)\n",
    "    print(\"final point: \", x)\n",
    "    print(\"final function value: \", f(x))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(\"hybrid newton\")\n",
    "    x, x_all,i = hybrid_newton(f, g, h, point, s, alpha, beta)\n",
    "    print(\"number of iterations: \", i)\n",
    "    print(\"final point: \", x)\n",
    "    print(\"final function value: \", f(x))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
