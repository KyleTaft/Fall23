{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"cola\")\n",
    "dataset = dataset[\"train\"]  # Just take the training split for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenized_data = tokenizer(dataset[\"sentence\"], return_tensors=\"np\", padding=True)\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "tokenized_data = dict(tokenized_data)\n",
    "\n",
    "labels = np.array(dataset[\"label\"])  # Label is already an array of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-02 21:19:30.128910: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-02 21:19:31.568752: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at cola_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and compile our model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "# Lower learning rates are often better for fine-tuning transformers\n",
    "model.compile(optimizer=Adam(3e-5))  # No loss argument!\n",
    "\n",
    "model.fit(tokenized_data, labels)\n",
    "\n",
    "# save model\n",
    "model.save_pretrained(\"cola_model\")\n",
    "tokenizer.save_pretrained(\"cola_model_tokenizer\")\n",
    "\n",
    "# # # load model\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(\"cola_model\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"cola_model_tokenizer\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.02456144 0.9754386 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[0.806944 0.193056]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# test model\n",
    "test_sentence = \"This is a good sentence.\"\n",
    "tokenized_test_sentence = tokenizer(test_sentence, return_tensors=\"tf\")\n",
    "print(tf.nn.softmax(model(tokenized_test_sentence).logits))\n",
    "# test model\n",
    "test_sentence = \"This a sentence bad.\"\n",
    "tokenized_test_sentence = tokenizer(test_sentence, return_tensors=\"tf\")\n",
    "print(tf.nn.softmax(model(tokenized_test_sentence).logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 [==============================] - 106s 375ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'matthews_correlation': 0.7456700972528084}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code from https://huggingface.co/spaces/evaluate-metric/glue\n",
    "from evaluate import load\n",
    "glue_metric = load(\"glue\", \"cola\")\n",
    "glue_metric.compute(predictions=model.predict(tokenized_data).logits.argmax(axis=1), references=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
