{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in MNIST dataset\n",
      "X_train.shape:  (60000, 28, 28)\n",
      "Y_train.shape:  (60000,)\n",
      "X_test.shape:  (10000, 28, 28)\n",
      "Y_test.shape:  (10000,)\n",
      "\n",
      "FLATTEN\n",
      "X_train.shape:  (784, 60000)\n",
      "X_test.shape:  (784, 10000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras #JUST FOR THE MNIST DATASET (And the one-hot encoding)\n",
    "\n",
    "# Load Digit MNIST Dataset\n",
    "mnist = keras.datasets.mnist\n",
    "\n",
    "# Recover training & testing splits\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "print(\"Read in MNIST dataset\")\n",
    "print(\"X_train.shape: \", X_train.shape)\n",
    "print(\"Y_train.shape: \", Y_train.shape)\n",
    "print(\"X_test.shape: \", X_test.shape)\n",
    "print(\"Y_test.shape: \", Y_test.shape)\n",
    "\n",
    "# Flatten the training & testing data\n",
    "print(\"\")\n",
    "print(\"FLATTEN\")\n",
    "X_train = X_train.reshape((60000, 28*28))\n",
    "X_test = X_test.reshape((10000, 28*28))\n",
    "\n",
    "#Transpose the images\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "\n",
    "\n",
    "print(\"X_train.shape: \", X_train.shape)\n",
    "print(\"X_test.shape: \", X_test.shape)\n",
    "\n",
    "\n",
    "# Normalize the training & testing data\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "m = 60000 # Number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation Calculations\n",
    "\n",
    "We first define our variables and functions. \n",
    "- x: input vector \n",
    "- y: predicted output\n",
    "- w1: weight vector between input and hidden layer\n",
    "- w2: weight vector between hidden and output layer\n",
    "- b1: bias vector for hidden layer\n",
    "- b2: bias vector for output layer\n",
    "- z1: weighted sum of input and hidden layer -> $z_1 = w_1^Tx + b_1$\n",
    "- z2: weighted sum of hidden and output layer -> $z_2 = w_2^Ta_1 + b_2$\n",
    "- a1: activation of hidden layer (ReLU) -> $a_1 = \\begin{cases} z_1 & z_1 > 0 \\\\ 0 & z_1 \\leq 0 \\end{cases}$\n",
    "- a2: activation of output layer (Softmax) -> $a_2 = \\frac{e^{z_2}}{\\sum_{i=1}^n e^{z_2}}$\n",
    "- C0: cross-entropy loss -> $C_0 = -\\sum_{i=1}^n y_i \\log a_{2i}$\n",
    "\n",
    "- $\\frac{\\partial C_0}{\\partial z_2}$ #From Piazza @121_f1\n",
    "\n",
    "$\\frac{\\partial C_0}{\\partial z_2} = (a_2 - y)$\n",
    "; $\\frac{\\partial z_2}{\\partial w_2} = a_1$\n",
    "; $\\frac{\\partial z_2}{\\partial b_2} = 1$\n",
    "; $\\frac{\\partial z_2}{\\partial a_1} = w_2$\n",
    "; $\\frac{\\partial a_1}{\\partial z_1} = \\begin{cases} 1 & z_1 > 0 \\\\ 0 & z_1 \\leq 0 \\end{cases}$\n",
    "; $\\frac{\\partial z_1}{\\partial w_1} = x$\n",
    "; $\\frac{\\partial z_1}{\\partial b_1} = 1$\n",
    "; $\\frac{\\partial z_1}{\\partial x} = w_1$\n",
    "\n",
    "\n",
    "Where we update our weights and biases using gradient descent:\n",
    "\n",
    "- $w_1 = w_1 - t \\frac{\\partial C_0}{\\partial w_1}$\n",
    "- $w_2 = w_2 - t \\frac{\\partial C_0}{\\partial w_2}$\n",
    "- $b_1 = b_1 - t \\frac{\\partial C_0}{\\partial b_1}$\n",
    "- $b_2 = b_2 - t \\frac{\\partial C_0}{\\partial b_2}$\n",
    "\n",
    "Where $t$ is the learning rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialWeights(neurons):\n",
    "    # We subtract 0.5 to center the weights around 0\n",
    "\n",
    "    # Input -> First Hidden Layer\n",
    "    w1 = np.random.rand(neurons, 784) - 0.5 # 784 features\n",
    "    b1 = np.random.rand(neurons, 1) - 0.5\n",
    "\n",
    "    # First Hidden Layer -> Output\n",
    "    w2 = np.random.rand(10, neurons) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "def ReLU(x):\n",
    "    # if x > 0:\n",
    "    #     return x\n",
    "    # else:\n",
    "    #     return 0\n",
    "    return np.maximum(0, x) #vectorized\n",
    "\n",
    "def diffReLU(x):\n",
    "    # if x > 0:\n",
    "    #     return 1\n",
    "    # else:\n",
    "    #     return 0\n",
    "    return x > 0 #vectorized\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "def ForwardPass(x, w1, b1, w2, b2):\n",
    "    z1 = np.dot(w1, x) + b1 # Weight matrix * input vector + bias vector\n",
    "    a1 = ReLU(z1) # Apply activation function\n",
    "    z2 = np.dot(w2, a1) + b2 # Weight matrix * input vector + bias vector\n",
    "    a2 = softmax(z2) # Apply softmax\n",
    "    return z1, a1, z2, a2 # Return all intermediate values for backprop & final output\n",
    "\n",
    "def BackProp(x, y, z1, a1, z2, w2, a2):\n",
    "    m = y.shape[0] # Number of training examples\n",
    "\n",
    "    #One-hot encode the labels\n",
    "    one_hot_Y =  keras.utils.to_categorical(y).T\n",
    "   \n",
    "    # dC0/dz2 = a2 - y\n",
    "    dz2 = a2 - one_hot_Y\n",
    "    \n",
    "    # dC0/dw2 = dC0/dz2 * a1\n",
    "    dw2 = np.dot(dz2, a1.T) / m\n",
    "\n",
    "    # dC0/db2 = dC0/dz2 * 1\n",
    "    db2 = np.sum(dz2) / m #average to match shape of bias\n",
    "\n",
    "    # dC0/dz1 = dC0/dz2 * w2 * diffReLU(z1)\n",
    "    dz1 = np.dot(w2.T, dz2) * diffReLU(z1)\n",
    "\n",
    "    # dC0/dw1 = dC0/dz1 * x\n",
    "    dw1 = np.dot(dz1,x.T) / m\n",
    "\n",
    "    # dC0/db1 = dC0/dz1 * 1\n",
    "    db1 = np.sum(dz1) / m #average to match shape of bias\n",
    "\n",
    "    return dw1, db1, dw2, db2\n",
    "\n",
    "\n",
    "def updateWeights(w1, b1, w2, b2, dw1, db1, dw2, db2, lr):\n",
    "    # Apply gradient descent\n",
    "    w1 = w1 - lr * dw1\n",
    "    b1 = b1 - lr * db1\n",
    "    w2 = w2 - lr * dw2\n",
    "    b2 = b2 - lr * db2\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "def accuracy(a2, y):\n",
    "    # number of correct predictions / total number of predictions\n",
    "    return np.sum(np.argmax(a2, axis=0) == y) / y.size \n",
    "\n",
    "def fit(X, Y, iterations, lr):\n",
    "    w1, b1, w2, b2 = initialWeights(neurons = 256)\n",
    "    for i in range(1,iterations+1):\n",
    "        z1, a1, z2, a2 = ForwardPass(X_train, w1, b1, w2, b2)\n",
    "        dw1, db1, dw2, db2 = BackProp(X, Y, z1, a1, z2, w2, a2)\n",
    "        w1, b1, w2, b2 = updateWeights(w1, b1, w2, b2, dw1, db1, dw2, db2, lr)\n",
    "        if i % 25 == 0 or i == 1:\n",
    "            print(\"iter\", i)\n",
    "            print(\"Accuracy:\",accuracy(a2, Y))  \n",
    "    return w1, b1, w2, b2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1\n",
      "Accuracy: 0.05705\n",
      "iter 25\n",
      "Accuracy: 0.5424333333333333\n",
      "iter 50\n",
      "Accuracy: 0.7780666666666667\n",
      "iter 75\n",
      "Accuracy: 0.8331166666666666\n",
      "iter 100\n",
      "Accuracy: 0.8582666666666666\n",
      "iter 125\n",
      "Accuracy: 0.8450666666666666\n",
      "iter 150\n",
      "Accuracy: 0.8846833333333334\n",
      "iter 175\n",
      "Accuracy: 0.8935333333333333\n",
      "iter 200\n",
      "Accuracy: 0.9004333333333333\n",
      "iter 225\n",
      "Accuracy: 0.9061\n",
      "iter 250\n",
      "Accuracy: 0.9107\n"
     ]
    }
   ],
   "source": [
    "parameters = fit(X_train, Y_train, 250, 0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9097\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "z1, a1, z2, a2 = ForwardPass(X_test, parameters[0], parameters[1], parameters[2], parameters[3])\n",
    "print(\"Accuracy:\",accuracy(a2, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
