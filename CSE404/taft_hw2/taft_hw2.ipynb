{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "state": "normal"
   },
   "outputs": [],
   "source": [
    "# CSE 404 Introduction to Machine Learning (Fall 2023)\n",
    "# Homework 2 \n",
    "#\n",
    "# Perceptron Learning Algorithm (PLA)\n",
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "state": "normal"
   },
   "outputs": [],
   "source": [
    "# Import all the things\n",
    "import time\n",
    "import numpy as np                    #vectors, matrices, linear algebra...\n",
    "from random import choice             #randomly select item from list\n",
    "from sklearn.utils import shuffle     # shuffle randomly reorders the values\n",
    "import matplotlib.pyplot as plt       #plots/graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "state": "normal"
   },
   "outputs": [],
   "source": [
    "# For Perceptron\n",
    "# Trains a perceptron model given a training dataset\n",
    "# @training_data :: List of data points, \n",
    "#                   where training_data[0] contains data points\n",
    "#                   & training_data[1] contains labels {-1, +1}\n",
    "# Returns: Learned model vector\n",
    "\n",
    "def train_perceptron(training_data):\n",
    "    X = training_data[0]     # dataset\n",
    "    y = training_data[1]     # labels\n",
    "    model_size = X.shape[1]  # num columns; X.shape[0] --> # rows\n",
    "    w = np.zeros(model_size) # zeroed column vector, size of model\n",
    "                             # Random init: np.random.rand(model_size)\n",
    "    \n",
    "    iteration = 1\n",
    "    # while True, do PLA steps\n",
    "    while iteration < 1000:\n",
    "        # compute results according to the hypothesis\n",
    "\n",
    "        y_hat = np.sign(np.dot(X, w)) \n",
    "\n",
    "        \n",
    "        # get incorrect predictions (can use the indices)\n",
    "\n",
    "        misclassified = np.where(y != y_hat)[0] #where there is a mismatch\n",
    "         \n",
    "\n",
    "        # Check the convergence criteria (if there are no misclassified points, the PLA is converged and we can stop)\n",
    "        if misclassified.size == 0:\n",
    "            break\n",
    "        \n",
    "        # Pick one misclassified example (x*,y*)\n",
    "        x_star = X[misclassified[0]]\n",
    "        y_star = y[misclassified[0]]\n",
    "         \n",
    "\n",
    "        # Update the weight vector with perceptron update rule\n",
    "        w = w + x_star*y_star # w_t+1 = w_t + x* * y*\n",
    "        \n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    return w     # return weight vector (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "state": "normal"
   },
   "outputs": [],
   "source": [
    "# For Perceptron\n",
    "# Print the predictions given a dataset & learned model\n",
    "# @model :: model (weight) vector\n",
    "# @data :: data points\n",
    "# Returns: none\n",
    "\n",
    "def print_prediction(model,data):\n",
    "    result = np.matmul(data,model)\n",
    "    predictions = np.sign(result)\n",
    "    for i in range(len(data)):\n",
    "        print(\"{}: {} -> {}\".format(data[i][:2], result[i], predictions[i]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "state": "normal"
   },
   "outputs": [],
   "source": [
    "# For Linear Regression\n",
    "# Randomly split the dataset into training & testing sets\n",
    "# @train_perc :: Percentage (in decimal format) of data to use for training\n",
    "#                Example: if train_perc == 0.7 --> 70% training, 30% testing\n",
    "# @data :: features\n",
    "# @label :: targets\n",
    "# Returns training data, testing data, training labels, testing labels\n",
    "\n",
    "def rand_split_train_test(data, label, train_perc):\n",
    "    if train_perc >= 1 or train_perc <= 0:\n",
    "        raise Exception('train_perc should be between (0,1).')\n",
    "    sample_size = data.shape[0]     # num rows \n",
    "    if sample_size < 2:\n",
    "        raise Exception('Sample size should be larger than 1. ')\n",
    "\n",
    "    num_train_sample = np.max([np.floor(sample_size * train_perc).astype(int), 1])\n",
    "    data, label = shuffle(data, label)\n",
    "\n",
    "    data_tr = data[:num_train_sample]     \n",
    "    data_te = data[num_train_sample:]     \n",
    "\n",
    "    label_tr = label[:num_train_sample]     \n",
    "    label_te = label[num_train_sample:]     \n",
    "\n",
    "    return data_tr, data_te, label_tr, label_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Linear Regression\n",
    "# Takes a subsample of the entire dataset\n",
    "# Takes subsample_size chunk of X & y within bounds\n",
    "\n",
    "def subsample_data(data, label, subsample_size):\n",
    "    # protected sample size\n",
    "    subsample_size = np.max([1, np.min([data.shape[0], subsample_size])])\n",
    "    data, label = shuffle(data, label)\n",
    "    data = data[:subsample_size]\n",
    "    label = label[:subsample_size]\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Linear Regression\n",
    "# Generates a random dataset with dimensions based on feature_size & sample_size\n",
    "# @feature_size == x_i # columns\n",
    "# @sample_size == # rows\n",
    "# @bias :: True --> add Gaussian noise\n",
    "\n",
    "def generate_rnd_data(feature_size, sample_size, bias=False):\n",
    "    # Generate X matrix\n",
    "    data = np.concatenate((np.random.randn(sample_size, feature_size), np.ones((sample_size, 1))), axis=1) \\\n",
    "        if bias else np.random.randn(sample_size, feature_size)  # the first dimension is sample_size (n X d)\n",
    "\n",
    "    # Generate ground truth model\n",
    "    # If bias == T: then #rows + 1\n",
    "    truth_model = np.random.randn(feature_size + 1, 1) * 10 \\\n",
    "        if bias else np.random.randn(feature_size, 1) * 10\n",
    "\n",
    "    # Generate labels\n",
    "    label = np.dot(data, truth_model)\n",
    "\n",
    "    # Add element-wise Gaussian noise to each label\n",
    "    label += np.random.randn(sample_size, 1)\n",
    "    return data, label, truth_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Linear Regression\n",
    "# Sine Function :)\n",
    "\n",
    "def sine_data(sample_size, order_M, plot_data = False, noise_level = 0.1, bias = False):\n",
    "    if int(order_M) != order_M: \n",
    "        raise Exception('order_M should be an integer.')\n",
    "    if order_M < 0:\n",
    "        raise Exception('order_M should be at least larger than 0.')\n",
    "    \n",
    "    # Generate X matrix\n",
    "    x = np.random.rand(sample_size,1) * 2 * np.pi        # generate x from 0 to 2pi\n",
    "    X = np.column_stack([ x**m for m in range(order_M)])\n",
    "\n",
    "    data = np.concatenate((X, np.ones((sample_size, 1))), axis=1) if bias else X\n",
    "\n",
    "    # Ground truth model: a sine function\n",
    "    f = lambda x: np.sin(x)\n",
    "\n",
    "    # Generate labels\n",
    "    label = f(x)\n",
    "\n",
    "    # Add element-wise Gaussian noise to each label\n",
    "    label += np.random.randn(sample_size, 1)*noise_level\n",
    "\n",
    "    if plot_data:\n",
    "        plt.figure()\n",
    "        xx = np.arange(0, np.pi * 2, 0.001)\n",
    "        yy = f(xx)\n",
    "        plt.plot(xx, yy, linestyle = '-', color = 'g', label = 'Objective Value')\n",
    "        plt.scatter(x, label, color = 'b', marker = 'o', alpha = 0.3)\n",
    "        plt.xlabel(\"t\")\n",
    "        plt.ylabel(\"x\")\n",
    "        plt.title(\"Sine Data (N = %d) with Noise Level %.4g.\".format(sample_size, noise_level))\n",
    "        plt.show()\n",
    "\n",
    "    return data, label, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Linear Regression\n",
    "# Compute the mean square error (MSE) between the true and predicted labels\n",
    "# @true_label :: y --> Nx1 vector\n",
    "# @predicted_label :: y^ --> Nx1 vector\n",
    "# Returns :: scalar MSE value\n",
    "\n",
    "def mean_squared_error(true_label, predicted_label):\n",
    "    #TODO\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Linear Regression\n",
    "# Compute the model vector obtained after MLE\n",
    "# w_star = (X^T X)^(-1)X^T t\n",
    "# @feature :: Nx(d+1) matrix\n",
    "# @target :: Nx1 vector\n",
    "# Returns :: w_star --> (d+1)x1 model vector\n",
    "\n",
    "def least_squares(feature, target):\n",
    "    #TODO\n",
    "    return w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Linear Regression\n",
    "# Compute the model vector when using L2-norm regularization\n",
    "# w_star = (X^T X + LI)^(-1) X^T t\n",
    "# @feature :: Nx(d+1) matrix\n",
    "# @target :: Nx1 vector\n",
    "# @lam :: scalar regularization parameter, lambda\n",
    "# Returns :: w_star --> (d+1)x1 model vector\n",
    "\n",
    "def ridge_regression(feature, target, lam = 1e-17):\n",
    "    #TODO\n",
    "    return w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Linear Regression\n",
    "# k-fold Cross Validation\n",
    "\n",
    "def k_fold_cross_validation(current_fold, total_fold, total_sample_size):\n",
    "    #TODO\n",
    "    return data_train, data_test, label_train, label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "state": "normal"
   },
   "outputs": [],
   "source": [
    "# MAIN \n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # PERCEPTRON MAIN #########################################\n",
    "    rnd_x = np.array([[0, 1, 1], \\\n",
    "                      [0.6, 0.6, 1], \\\n",
    "                      [1, 0, 1], \\\n",
    "                      [1, 1, 1], \\\n",
    "                      [0.3, 0.4, 1], \\\n",
    "                      [0.2, 0.3, 1], \\\n",
    "                      [0.1, 0.4, 1], \\\n",
    "                      [0.5, -0.1, 1]])\n",
    "\n",
    "    rnd_y = np.array([1, 1, 1, 1, -1, -1, -1, -1])\n",
    "    rnd_data = [rnd_x, rnd_y]\n",
    "\n",
    "    trained_model = train_perceptron(rnd_data)\n",
    "    print(\"Model:\", trained_model)\n",
    "    print_prediction(trained_model, rnd_x)\n",
    "    # TODO :: Add functions above to solve Q2 (Problem 1.4 (a-e) in LFD) \n",
    "    \n",
    "    ###########################################################\n",
    "    # LINEAR REGRESSION MAIN (TODO)\n",
    "    #\n",
    "    # plt.interactive(False)\n",
    "    # np.random.seed(404)\n",
    "\n",
    "    # Complete Least Squares, Ridge Regression, MSE functions\n",
    "    # Randomly generate & plot 30 data points using sine function\n",
    "    # Randomly split the dataset\n",
    "    # For each lambda, use Ridge Regression to calculate & plot MSE for training & testing sets\n",
    "    # Implement k-fold CV & choose best lambda \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "mimir": {
   "data": {},
   "last_submission_id": "",
   "project_id": "0e138e9d-41b8-46dd-9b77-1101748315bf"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
