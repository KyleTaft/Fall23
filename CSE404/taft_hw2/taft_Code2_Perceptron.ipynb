{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "state": "normal"
   },
   "source": [
    "# Code 2: Perceptron\n",
    "\n",
    "## Part 1: Scikit-Learn Overview\n",
    "Scikit-Learn uses the same design principles across its API. This is a brief overview of the format. \n",
    "\n",
    "### Consistency\n",
    "All objects share a consistent and simple interface.\n",
    "\n",
    "### Estimators\n",
    "Any object that estimates parameters based on a dataset is called an *estimator*. The estimation is done by the *fit( )* method which takes a dataset (and labels for a supervised algorithm) as a parameter. All other parameters used to guide the estimation are called *hyperparameters*, and these are set as instance variables via a constructor parameter. \n",
    "\n",
    "### Transformers\n",
    "Estimators that can also transform the dataset are called *transformers* (not to be confused with the newest Neural Net Transformer). This is done with the *transform( )* method on the dataset. All transformers also have the *fit_transform( )* method: this is equivalent to calling fit( ) and then transform( ), but is usually optimized to run faster. \n",
    "\n",
    "### Predictors\n",
    "Estimators that are given a dataset and can make predictions on it are called *predictors*. This is done with the *predict( )* method which takes a dataset and returns corresponding predictions. These also have a *score( )* method that measures the quality of the predictions using a test set (and corresponding labels if supervised). \n",
    "\n",
    "### Inspection\n",
    "All the estimator's hyperparameters are accesible through public instance variables (e.g., imputer.strategy) and its learned parameters are accessible through public instance variables with an underscore suffix (e.g., imputer.statistics_). \n",
    "\n",
    "### Use of other packages\n",
    "Scikit-Learn uses other packages rather than create its own classes. Datasets are represented as NumPy arrays or SciPy matrices. Hyperparameters are Python strings or numbers. \n",
    "\n",
    "### Default Parameters\n",
    "Scikit-Learn starts with typical default values for most parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "state": "normal"
   },
   "source": [
    "## Part 2: Basic Perceptron\n",
    "Let's code a perceptron using Scikit-Learn!\n",
    "Your job is to consult the Scikit-Learn API and fill in the skeleton code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "spicy_tyr",
    "state": "normal"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "eager_nott",
    "starter_code": "# Load the iris dataset\n# It consists of 3 different types of irises’ (Setosa, Versicolour, & Virginica) \n#     petal & sepal length, stored in a 150x4 numpy.ndarray.\n#     Rows: samples \n#     Columns: Sepal Length, Sepal Width, Petal Length & Petal Width.\niris = ",
    "state": "graded"
   },
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "# It consists of 3 different types of irises’ (Setosa, Versicolour, & Virginica) \n",
    "#     petal & sepal length, stored in a 150x4 numpy.ndarray.\n",
    "#     Rows: samples \n",
    "#     Columns: Sepal Length, Sepal Width, Petal Length & Petal Width.\n",
    "iris = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "red_var",
    "starter_code": "# Split the dataset into X (features) and y (labels)\n# X: should contain all rows, but only the petal length & petal width features\nX = \n\n# y: Setosa is our target\ny = \n\nprint(X)\nprint(y)",
    "state": "graded"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into X (features) and y (labels)\n",
    "# X: should contain all rows, but only the petal length & petal width features\n",
    "X = \n",
    "\n",
    "# y: Setosa is our target\n",
    "y = \n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "big_hel",
    "starter_code": "# Create an instance of a Perceptron classifier\nper_clf = \n",
    "state": "graded"
   },
   "outputs": [],
   "source": [
    "# Create an instance of a Perceptron classifier\n",
    "per_clf = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "calm_tyr",
    "starter_code": "# Use the Perceptron's fit method on your smaller dataset\nper_clf.",
    "state": "graded"
   },
   "outputs": [],
   "source": [
    "# Use the Perceptron's fit method on your smaller dataset\n",
    "per_clf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "blue_oor",
    "starter_code": "# Use the Perceptron's predict method on sample[2,0.5]\ny_pred = \n\nprint(y_pred)",
    "state": "graded"
   },
   "outputs": [],
   "source": [
    "# Use the Perceptron's predict method on sample[2,0.5]\n",
    "y_pred = \n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "state": "normal"
   },
   "source": [
    "## Part 3: Another Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "blue_ran",
    "state": "read_only"
   },
   "outputs": [],
   "source": [
    "# import Scikit-Learn's StandardScaler, train_test_split, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "stoic_vali",
    "starter_code": "# Save the iris dataset data and target values to X and y, respectively\nX = \ny = ",
    "state": "graded"
   },
   "outputs": [],
   "source": [
    "# Save the iris dataset data and target values to X and y, respectively\n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "green_vali",
    "starter_code": "# Print/view the first 5 observations from y\n",
    "state": "graded"
   },
   "outputs": [],
   "source": [
    "# Print/view the first 5 observations from y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "blue_magni",
    "starter_code": "# Print/view the first 10 observations of X\n",
    "state": "graded"
   },
   "outputs": [],
   "source": [
    "# Print/view the first 10 observations of X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "green_mani",
    "starter_code": "# Split the dataset into 80% training and 20% testing\nX_train, X_test, y_train, y_test = ",
    "state": "graded"
   },
   "outputs": [],
   "source": [
    "# Split the dataset into 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "gray_sol",
    "starter_code": "# Train the StandardScaler to format the X training set\n# Remember StandardScaler standardizes all features to have\n#     mean = 0 and unit variance\nsc = \nsc.",
    "state": "graded"
   },
   "outputs": [],
   "source": [
    "# Train the StandardScaler to format the X training set\n",
    "# Remember StandardScaler standardizes all features to have\n",
    "#     mean = 0 and unit variance\n",
    "sc = \n",
    "sc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "calm_sif",
    "starter_code": "# Apply the StandardScaler to the X training dataset\nX_train_std = \n\n# Apply StandardScaler to X test data\nX_test_std = \n",
    "state": "graded"
   },
   "outputs": [],
   "source": [
    "# Apply the StandardScaler to the X training dataset\n",
    "X_train_std = \n",
    "\n",
    "# Apply StandardScaler to X test data\n",
    "X_test_std = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "fit_thor",
    "starter_code": "# Create a Perceptron object with parameters:\n#     50 iterations (epochs) over the dataset\n#     learning rate n = 0.1\n#     random_state = 0\nper = \n\n# Train the Perceptron on the standardized X training set\nper.",
    "state": "graded"
   },
   "outputs": [],
   "source": [
    "# Create a Perceptron object with parameters:\n",
    "#     50 iterations (epochs) over the dataset\n",
    "#     learning rate n = 0.1\n",
    "#     random_state = 0\n",
    "per = \n",
    "\n",
    "# Train the Perceptron on the standardized X training set\n",
    "per."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "aged_saga",
    "starter_code": "# Apply trained Perceptron on standardized X test dataset to make predictions for y\ny_pred = \n\n# Print predictions\n",
    "state": "graded"
   },
   "outputs": [],
   "source": [
    "# Apply trained Perceptron on standardized X test dataset to make predictions for y\n",
    "y_pred = \n",
    "\n",
    "# Print predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "sly_odin",
    "starter_code": "# Print true labels\n\n",
    "state": "graded"
   },
   "outputs": [],
   "source": [
    "# Print true labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "green_nott",
    "starter_code": "# Print the accuracy_score of the model \n# You should compare the actual labels with the predicted labels\nprint(\"Accuracy: %.2f\" )",
    "state": "graded"
   },
   "outputs": [],
   "source": [
    "# Print the accuracy_score of the model \n",
    "# You should compare the actual labels with the predicted labels\n",
    "print(\"Accuracy: %.2f\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "mimir": {
   "data": {},
   "last_submission_id": "",
   "project_id": "6ea053f9-2d48-42db-8107-a05feddc3ec3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
