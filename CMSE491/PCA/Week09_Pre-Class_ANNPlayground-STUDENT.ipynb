{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9onE2l9X_mUO"
   },
   "source": [
    "# Week 09: Pre-Class Assignment: ANN Playground\n",
    "\n",
    "### <p style=\"text-align: right;\"> &#9989; Kyle Taft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![play](https://storage.googleapis.com/gweb-cloudblog-publish/images/neural-networks-106qyc.max-700x700.PNG)\n",
    "\n",
    "### Goals for this week's pre-class assignment\n",
    "\n",
    "In this Pre-Class Assignment you are going to use play around with neural networks in order to build your intuition of what they actually do.\n",
    "\n",
    "\n",
    "Total number of points: **18 points (2 points per question)**\n",
    "\n",
    "**This assignment is due by 11:59 p.m. the day before class,** and should be uploaded into the appropriate \"Pre-Class Assignments\" submission folder on D2L.  Submission instructions can be found at the end of the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DG3U3fRUseDy"
   },
   "source": [
    "---\n",
    "## Part 0: Reading\n",
    "\n",
    "Read chapters 10 and 11 of your textbook. Read through the questions at the end of each chapter and the answers in the Appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0oilBBi87NO"
   },
   "source": [
    "---\n",
    "\n",
    "## Part 1: Neural Network Intuition\n",
    "\n",
    "All ML algorithms require you to make choices about how to use the algorithm:\n",
    "\n",
    "* the number of neighbors in kNN,\n",
    "* the choice and properties (e.g., width $\\sigma$) of basis functions in RBFs,\n",
    "* regularization type (L1, L2, EN) and strength ($\\lambda$),\n",
    "* and so on.\n",
    "\n",
    "ANNs in particular have a large number of choices that you need to make, and it is not easy to know how to make those choices. \n",
    "\n",
    "In this problem you are going to build your intuition about how to make such choices and explain this intuition. To do this, you will run a large number of ANNs with various datasets, with various inputs, for various depths, for various widths, observing the contents of the hidden layers, examining the optimization process and so on. You can't really build your intuition for ANNs without spending **a lot** of time varying all of these choices together simulataneously. No pain, no gain.\n",
    "\n",
    "Fortunately, there is an excellent web app for doing just this.\n",
    "\n",
    "\n",
    "&#9989; **Task:** Go to [this](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.30258&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false) webpage. You will see a dashboard with:\n",
    "* choice of four datasets at the upper left,\n",
    "* below that you see training choices,\n",
    "* at the very right you can see the current output of the NN,\n",
    "* at the bottom right you can change what is being displayed - training versus testing, for example,\n",
    "* along the top you have controls for the regularization, activiation, learing rate and problem type,\n",
    "* note that if you switch the problem type to regression, the choice of datasets at the upper left changes.\n",
    "\n",
    "That is a lot to vary! And, we have not yet discussed the most interesting part in the center: **the ANN itself**. Note that you also have control over these properties of the NN:\n",
    "* along the top of the ANN you can change the number of hidden layers - this is **true** deep learning!\n",
    "* you can do feature engineering by choosing what features you use as inputs on the left,\n",
    "* you can change the width of the NN with the +/- buttons above each hidden layer.\n",
    "\n",
    "You run the ANN with the button at the very upper left, and it will generate a running plot at the upper right with the loss function. Note that to the left of the \"play\" button is a reset button. And, as it runs you get a view of what is in each \"neuron\" in the hidden layers. You really get to see everything that is going on.\n",
    "\n",
    "\n",
    "Now, you are going to follow these steps and answer these questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 1.1:** Reduce the size of the ANN to its minimum size: input only $X_1$, one hidden layer, and one neuron in that hidden layer. For each of the four classification datasets run the ANN, remembering to reset it every time. Then, do this with *only* $X_2$. Be sure that regularization is set to `None` for now.  Describe what you see here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=\"#009600\">&#9998;</font> *Put your answers here!* \n",
    "\n",
    "**With the input being only the x1 the model finds the vertical line that best separates the data. With the input being only the x2 the model finds the horizontal line that best separates the data.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 1.2:** Next, repeat what you just did for each of the other possible inputs, from $X_1$ to $\\sin(X_2)$ using only one input at a time. Remember to reset every time. Describe the behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=\"#009600\">&#9998;</font> *Put your answers here!* \n",
    "\n",
    "- **X1: The model finds the vertical line that best separates the data.**\n",
    "- **X2: The model finds the horizontal line that best separates the data.**\n",
    "- **X1^2: The model finds the two vertical lines that best separates the data.**\n",
    "- **X2^2: The model finds the two horizontal lines that best separates the data.**\n",
    "- **X1*X2: The model finds the diagonal section that best separates the data.**\n",
    "- **sin(X1): The model finds the two vertical sections (4 lines) that best separates the data.**\n",
    "- **sin(X2): The model finds the two horizontal section (4 lines) that best separates the data.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 1.3:** You are getting the idea: now, vary all of the inputs in many different combinations. This is a form of _feature engineering_ where you, the user, gets to control what the NN gets trained on. Describe the patterns you see and what conclusions about feature engineering you would draw from this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=\"#009600\">&#9998;</font> *Put your answers here!* \n",
    "\n",
    "**The model will find the best way to separate the data based on the inputs. The more inputs the more complex the model can be. For example if we allow for X1 and X2 we have the two degrees of freedom to build a diagonal line. Adding more transformed inputs allows for more complicated structures than this**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 1.4:**  Reset everything and choose the first feature $X_1$ again, still with only one hidden layer. One by one add neurons to the hidden layer and describe what happens. Be sure to do all of these tests for the four datasets and comment on which ones are easier for the NN and which are harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=\"#009600\">&#9998;</font> *Put your answers here!* \n",
    "\n",
    "\n",
    "As we add more neurons the model can use more vertical lines to separate the data. The more neurons the more complex the model can be.\n",
    "\n",
    "- **Circle: Adding additional neurons allows for the model to fit much better than one**\n",
    "- **XOR: No improvment can be made from only one neuron due to how the data cannot be split vertically any better**\n",
    "- **Gaussian: No improvment can be made since the simple one neuron already seperates the data**\n",
    "- **Spiral: Adding the additional neurons helps the model fit the sections better**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 1.5:**  Now reset, use the same input with **two** hidden layers with only one neuron in each layer; you will need to remove a neuron in the second layer because it will try to put two there. After noticing what it does, put that second neuron in the second layer and compare. What did you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=\"#009600\">&#9998;</font> *Put your answers here!* \n",
    "\n",
    "**The hidden layer is taking in the vertical line the first neuron found and is trying (usually) a different seperator to see if it can improve the model. The use of backpropagration updates the weights according to how well it improves (or not) the output. Adding more neurons allows for more choices happening at one time.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 1.6:**  Build a deep and wide NN by adding layers with lots of neurons, but use *only* the first input $X_1$ and pay most attention to the data that has blue dots at the center surrounded by a ring of orange dots (upper left). Just using $X_1$ as an input can you build any NN to get the circular separation boundary needed? What if you add $X_2$ to the possible inputs as well? Describe the shape of the boundary. Also, this tool allows you to see what it in the hidden layers - what patterns do you see forming there? (If you hover your mouse over an internal neuron it expands it so that you can see it better.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=\"#009600\">&#9998;</font> *Put your answers here!* \n",
    "\n",
    "**Even making the model very wide and deep it cannot find a way to separate the data. This is because is we look at the data projected onto the x-axis (AKA only considering X1) the data is not seperable at all. Adding X2 allows for the model to find the circular boundary. It is interesting to see the variation that the # of neurons and layers have on the shape of this seperator. For a very deep model it converges much faster and seems to have a more circular shape from each layer into the next. The number of neurons allow for the handful of choices the model can make**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 1.7:**  Repeat step 6 with the two regularizers, varying their strength. What do you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=\"#009600\">&#9998;</font> *Put your answers here!* \n",
    "\n",
    "**As we increase the regularization strength of either L1 or L2 the weights are penelized and therefore many of them go to 0. This removes complexity from our model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 1.8:**  Click on one of the weights; that is, one of the lines that connects two neurons. After hovering, your click will open a box allowing you to change that particular weight. Change some of the weights to see what the consequences are. (It is more instructive if you change the weights by a lot so that you see a bigger impact.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=\"#009600\">&#9998;</font> *Put your answers here!* \n",
    "\n",
    "**Changing the weights to be higher on certain links makes the model more represented/influences by that specific connection. This causes a big change in the output of the model. While we can also try to destroy what a model has learned by setting strong connections to 0, the model still seems to usually give good answers by relying on the other connections**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 1.9:**  Now, finally, focus on the dataset with the spiral - the one at the lower right. What is the minimal deep net you can construct that allows you to find a spiral separation boundary? Watch the graph at the upper right - does it appear to be hopping among various local minima?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=\"#009600\">&#9998;</font> *Put your answers here!* \\\n",
    "\n",
    "**Using only X1 and X2 without feature engineering the model struggles to fit the problem. The minimum that I found which is likely an upperlimit was three layers with (5,5,2) neurons. The loss curve does show there to be \"bumps\" along the way of the model falling into local minimia then discovering a better set of weights.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#169; Copyright 2023, Department of Computational Mathematics, Science and Engineering at Michigan State University."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
