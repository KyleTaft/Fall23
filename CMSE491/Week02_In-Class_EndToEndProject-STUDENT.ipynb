{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 02: In Class Assignment: End-to-End Project\n",
    "\n",
    "### <p style=\"text-align: right;\"> &#9989; Kyle Taft.\n",
    "<p style=\"text-align: right;\"> &#9989; Lucas, Dawit, Nicolai, Jacob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYJy2FQ6gqKv"
   },
   "source": [
    "![houses](https://www.mercurynews.com/wp-content/uploads/2017/04/oo24dg-web0409buslrealtrendsrisk02.jpg)\n",
    "\n",
    "\n",
    "<h1><center> <font color='green'>Machine Learning Housing Corp.</font></h1></center>\n",
    "\n",
    "This In Class Assignment completes what you have done in the Pre Class Assignment. If you haven't completed the Pre Class, do so now.\n",
    "\n",
    "\n",
    "As we did last time, follow these steps:\n",
    "1. read this notebook first so that you know what to expect for today\n",
    "2. answer the questions below\n",
    "3. turn in this notebook with your answers in the usual way (no need to resubmit the notebook from the textbook)\n",
    "\n",
    "Last time you explored the nature of the problem, what the data generally looked like and examined some properties, such as correlations and statistics (thanks to nice functionality in `pandas`). Now it is time to clean the data before it goes into ML algorithms.\n",
    "\n",
    "Each of the sections below follow through the process described in the textbook and the notebook that comes with it. Read through that notebook and follow the steps in there. As you work through the notebook, answer the questions below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CKgy65Zfxn-"
   },
   "source": [
    "\n",
    "## Part 1. Data Cleaning\n",
    "\n",
    "Let's think about the steps you have done so far:\n",
    "* You obtained the data. (This came to you through a link in notebook, which made this part easy!)\n",
    "* You examined the data using `pandas`.\n",
    "* You visualized the data in a few ways, including using maps and pairplots that reveal correlations among the features.\n",
    "* You hopefully noticed some characteristics of this data, including potential problems with it. \n",
    "\n",
    "Next, we want to use the data for ML, but we now must repair all of the problems - `sklearn` will not know what to do with erroneous data! This is a fairly important step in ML: _sometimes your data has errors, missing values or is simply represented in a way `sklearn` (or whatever you are using) can't process (e.g., string information, rather than floats)_. \n",
    "\n",
    "What to do? There are many approaches and generally you need to apply them in ways that depend on your specific problem. For example, suppose there is a row in your data that has a missing value. A simple fix is to simply remove that row. But, if you **really** need that data point and the other columns are perfectly fine, what should you do? Let's examine these questions in the context of the data you have been working with.\n",
    "\n",
    "1. Your dataset has missing values. Using only `pandas`, you can use the methods `dropna`, `drop` and `fillna` - write in a markdown cell what each of these accomplish and why you would use each of them. \n",
    "\n",
    "2. `sklearn` has a sub-package called `impute` that handles imputation.  \n",
    "Read this [`article`](https://scikit-learn.org/stable/modules/impute.html#impute) and answer these questions:\n",
    "\n",
    "    a. What does imputation mean?<br>\n",
    "    b. What strategies are there to handle missing data?<br>\n",
    "    c. What type of imputer does `sklearn` provide? What does each one of them do?\n",
    "\n",
    "3. Review the code below and in a markdown cell show the math behind it. \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(imp.transform(X))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.         2.        ]\n",
      " [6.         3.66666667]\n",
      " [7.         6.        ]\n",
      " [4.         3.66666667]\n",
      " [4.         3.66666667]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6], [np.nan, np.nan], [np.nan, np.nan]])\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6], [np.nan, np.nan], [np.nan, np.nan]]\n",
    "print(imp.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=\"#009600\">&#9998;</font> *Put your answers here!* \n",
    "\n",
    "1. \n",
    "    - dropna: drops rows with nans or missing values\n",
    "    - drop: drops rows with specified parameters\n",
    "    - fillna: fills in missing values with specified parameters\n",
    "\n",
    "\n",
    "\n",
    "2. \n",
    "    - a. Imputation is fill missing data by infering them from known parts of the data\n",
    "\n",
    "    - b. It provide univariate and multivariate imputers. This allows for filling of one or more missing values.\n",
    "\n",
    "    - c. There is substitute with constant, mean, median, mode, predict with regression, and KNN.\n",
    "\n",
    "3.\n",
    "\n",
    "        a. It takes the mean of the column of the fit data and replaces the missing values with the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9Cf_pnY6DKa"
   },
   "source": [
    "\n",
    "### 1.1 Data Cleaning with Categorical Data\n",
    "\n",
    "Let's move to more data cleaning, with a focus on categorical data and standard transformation/scaling operations. \n",
    "\n",
    "1. What is \"[one-hot coding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)\" and what it is used for?\n",
    "2. What are the defaults in [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)? \n",
    "3. Have a discussion in your group about transformations and scaling. What does this mean? Why is this important? \n",
    "4. Give an example, in which you would use one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=\"#009600\">&#9998;</font> *Put your answers here!* \n",
    "\n",
    "1. It turns catagorical data into true/false values. It is used to make catagorical data usable in machine learning.\n",
    "2. catagories: auto which figures out the catagories automatically, drop=None, sparse_output=True; gives sparse matrix output.\n",
    "3. Transformations are performing an action to change the data. Scaling changes the scale that the data ranges. Scaling is important because just because a number is bigger does not mean it is more important. Transformations are needed to make the data most compatible with the machine learning algorithm.\n",
    "4. When you have a catagory of colors and you want to use it for your algorithm. Numbering them does not make sense since there is not a numerical relationship between the colors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZpg0EV86tbW"
   },
   "source": [
    "## Part 2. Pipelines\n",
    "\n",
    "As you have likely gathered by now, the ML process can have a lot of steps. And, as you can see, the steps are fairly common across different ML settings. It would be very nice if `sklearn` provided some functionality to help organize some of these steps. \n",
    "\n",
    "\n",
    "1. Read about and summarize what [pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) are.\n",
    "2. Which transformations are used in the pipeline for this project?\n",
    "3. Is it important that the tranformers in the pipeline are done in a certain order? \n",
    "4. How does the author handle the exponential distributed data?\n",
    "5. How does the author handle multimodal distribution? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=\"#009600\">&#9998;</font> *Put your answers here!* \n",
    "\n",
    "1. Pipelines are a sequence of transformations that are applied to the data. They can be used to both organize the steps and be used again on other data.\n",
    "2. Imputer, OneHotEncoder, scaler, and other transformers. :-)\n",
    "3. Yes, it is important that the transformers are done in a certain order. The data must be imputed before it can be scaled.\n",
    "4. The authors takes the log of the data to make it more normally distributed.\n",
    "5. The author bucketizes the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pRCa0oh-aLX"
   },
   "source": [
    "___\n",
    "\n",
    "Now that you are done, follow these steps:\n",
    "* Submit **your** notebook to D2L.\n",
    "* Be sure to include the names of everyone in your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#169; Copyright 2023, Department of Computational Mathematics, Science and Engineering at Michigan State University."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ICA_Part_2_EndToEndProject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
