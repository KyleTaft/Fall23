{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSE 491 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Kyle Taft\n",
    "#### December 4, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/KyleTaft/Fall23/blob/main/CMSE491/gamma-radiation-clipart.png?raw=true\" alt=\"Gamma\" width=\"700\"/>\n",
    "\n",
    "[Source](https://www.pinclipart.com/maxpin/bioxxT/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTRUCTIONS FOR RUNNING THE NOTEBOOK\n",
    "\n",
    "* This notebook is used to train and evaluate models that extract nuclear decay energy peaks from 2D total absorption spectra (TAS) data. In order to produce enough data for machine learning, the data is simulated using the open source physics toolkit [Geant4](https://geant4.web.cern.ch/). Unfortunately, given the nature of the data being tens of thousands of matrices of size 512x512, the data is far too large to be attached to this notebook or uploaded to D2L in full. However, a small sample set of 15 matrices (5 single cascades, 5 double cascades, 5 two independent single cascades) and their corresponding labels are provided in the file `sample_data.npz` in the same folder as this notebook. The data can be loaded using the following code:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "data = np.load(\"sample_data.npz\")\n",
    "```\n",
    "\n",
    "* The data is stored in a dictionary, with the keys being the names of the matrices. The matrices are stored as numpy arrays, and can be accessed using the following code:\n",
    "\n",
    "```python\n",
    "matrix = data[\"matrix_name\"]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **tensorflow_addons**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Utilizing Machine Learning to Perform Total Absorption Spectroscopy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Motivation\n",
    "\n",
    "The analysis and interpretation of experimental data is an integral part of the scientific process, enabling us to uncover the underlying story held by the experiment. The method of processing this data has always gone hand in hand in utilization and improvement of cutting-edge technology. This is no exception in the modern era which is dominated by exponentially improving computer hardware and machine learning algorithms.\n",
    "\n",
    "In experimental physics, analyzing the spectrum of emitted light, a potent and accessible tool known as spectroscopy, serves to unravel the underlying physics governing an experiment, leading to valuable insights into the unknown phenomena at play. In nuclear physics research, spectroscopy is indispensable for analyzing the decay of exotic isotopes. This is because when an isotope decays into its daughter, it often leaves the new daughter isotope with an excess of energy. As a fundamental principle in physics, systems tend towards their lowest energy state and because of quantum mechanics, nuclides do not decay continuously, but instead emit discrete energy packets to achieve this favorable state of energy. This method of shedding energy can be through many means, such as ejecting a particle like a proton, neutron, or alpha particle, converting a proton or neutron into the other through beta decay, or through isomeric transition â€“ the nucleus simply releasing energy through light (gamma rays). While whole sections of nuclear physics are dedicated to creating a deep understanding of each one of these possible methods, the topic of this project specifically focuses on these gamma rays.\n",
    "\n",
    "When a nucleus emits a gamma ray it does so in accordance with selection rules and its own complex structure. This is all to say that this matter is not very simple nor predictable. Except for a subset of simplistic cases, often the isotope has multiple discrete excitation levels that each might have different paths of releasing gamma rays and reaching the ground state. To create a clearer explanation, I explain a fake example below.\n",
    "\n",
    "\n",
    "\n",
    "### EXAMPLE DECAY SCHEME\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/KyleTaft/Fall23/blob/main/CMSE491/Example%20Decay%20Scheme.png?raw=true\" alt=\"Decay\" width=\"400\"/>\n",
    "\n",
    "\n",
    "### EXPLAIN TAS WITH GEANT SIMS OF DECAY SCHEME\n",
    "\n",
    "\n",
    "### EXPLAIN DATA COLLECTION WITH SUN\n",
    "\n",
    "\n",
    "### EXPLAIN WHY THIS IS IMPORTANT (NUCLEAR  STRUCTURE, NUCLEAR ASTROPHYSICS, MEDICAL IMAGING, ETC.)\n",
    "\n",
    "### WHY MACHINE LEARNING IS IMPORTANT (AUTOMATION, SPEED, ETC.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "The goal of my project is to develop a model that accurately predicts the discrete energy levels, gamma-ray transitions, and level intensities of a given experimental or simulated nuclear decay spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Simulation and Preprocessing\n",
    "\n",
    "### Explain Geant4\n",
    "\n",
    "### Show sample input file\n",
    "\n",
    "### Show the attached sample data\n",
    "\n",
    "### How I make my labels\n",
    "\n",
    "### HONORS Superposition stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning Example\n",
    "import numpy as np\n",
    "def transform_label(label, single = True):\n",
    "    \"\"\"\n",
    "    label: a single label from the dataset\n",
    "    single: boolean, True if single, False if double\n",
    "    :return: the coordinates of energies in the label\n",
    "    \"\"\"\n",
    "    cord = np.unravel_index(np.argmax(label, axis=None), label.shape)[0:2] # find the max\n",
    "    if single:\n",
    "        cord = np.array([20*cord[1], 20*(511-cord[0])]) # convert to energy\n",
    "        return np.pad(cord, (0, 2), 'constant', constant_values=(-1,-1)) # pad with -1 to make it 2x2 matrix using unphysical values\n",
    "    else:\n",
    "        # find the second max\n",
    "        label[cord[0], cord[1]] = 0\n",
    "        cord2 = np.unravel_index(np.argmax(label, axis=None), label.shape)[0:2]\n",
    "        return np.array([20*cord[1], 20*(511-cord[0]), 20*cord2[1], 20*(511-cord2[0])])\n",
    "\n",
    "#Example labels\n",
    "single_label = np.load(\"/mnt/home/taftkyle/indiv_data/singles/single_test_label.npy\")[0]\n",
    "double_label = np.load(\"/mnt/home/taftkyle/indiv_data/doubles/double_test_label.npy\")[1]\n",
    "\n",
    "print(\"Single Label: \\n\", transform_label(single_label, ).reshape(2,2))\n",
    "print(\"Double Label: \\n\", transform_label(double_label, single = False).reshape(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data (example spectra)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to convert the matrix to energy values and counts:\n",
    "def matrix_to_coords(array):\n",
    "    xys = []\n",
    "    values = []\n",
    "    for i in range(array.shape[0]):\n",
    "        for j in range(array.shape[1]):\n",
    "            xys.append((20*j,20*(511-i)))\n",
    "            values.append(array[i,j])\n",
    "    return np.asarray(xys), np.asarray(values)\n",
    "\n",
    "# Load the first data point of test data:\n",
    "single = np.load(\"/mnt/home/taftkyle/indiv_data/singles/raw_single_test.npy\")[0]\n",
    "double = np.load(\"/mnt/home/taftkyle/indiv_data/doubles/raw_double_test.npy\")[1] # I loaded the second double because the first one is not as nice to visualize\n",
    "single_label= np.load(\"/mnt/home/taftkyle/indiv_data/singles/single_test_label.npy\")[0]\n",
    "double_label = np.load(\"/mnt/home/taftkyle/indiv_data/doubles/double_test_label.npy\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data:\n",
    "for example in [[single, single_label], [double, double_label]]:\n",
    "    # Separate the data and label:\n",
    "    data = example[0]\n",
    "    label = example[1]\n",
    "\n",
    "    #replace all values less than 1 with nan in order to appear white on graph\n",
    "    data = data.astype(float)\n",
    "    label = label.astype(float)\n",
    "    data[data < 1.] = np.nan\n",
    "    label[label < 1.] = np.nan\n",
    "\n",
    "    # Subplot 1: Plot the raw data\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(\"Raw Data\")\n",
    "    plt.xlabel(\"Individual Energy (keV)\")\n",
    "    plt.ylabel(\"Summed Energy (keV)\")\n",
    "    points, vals = matrix_to_coords(data)\n",
    "    plt.scatter(points[:,0],points[:,1], marker='s', c=vals, cmap='viridis', label=\"High Value Pixels\", s=1)\n",
    "    cbar = plt.colorbar()\n",
    "    plt.xlim(0,10000)\n",
    "    plt.ylim(0,10000)\n",
    "\n",
    "    # Subplot 2: Plot the labels\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title(\"Label\")\n",
    "    plt.xlabel(\"Individual Energy (keV)\")\n",
    "    plt.ylabel(\"Summed Energy (keV)\")\n",
    "    points, vals = matrix_to_coords(label)\n",
    "    plt.scatter(points[:,0],points[:,1], marker='s', c=vals, cmap='viridis', label=\"High Value Pixels\", s=10)\n",
    "    cbar = plt.colorbar()\n",
    "    plt.xlim(0,10000)\n",
    "    plt.ylim(0,10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My baseline model will be taking the two highest energy pixels\n",
    "\n",
    "# Note there is no need for a training set because we are not training a model\n",
    "\n",
    "# Load the data:\n",
    "single_test = np.load(\"/mnt/home/taftkyle/indiv_data/singles/raw_single_test.npy\")\n",
    "double_test = np.load(\"/mnt/home/taftkyle/indiv_data/doubles/raw_double_test.npy\")\n",
    "\n",
    "# Load the labels:\n",
    "single_test_labels = np.load(\"/mnt/home/taftkyle/indiv_data/singles/single_test_label.npy\")\n",
    "double_test_labels = np.load(\"/mnt/home/taftkyle/indiv_data/doubles/double_test_label.npy\")\n",
    "\n",
    "# Transform the labels:\n",
    "single_test_labels = np.array([transform_label(label) for label in single_test_labels])\n",
    "double_test_labels = np.array([transform_label(label, single = False) for label in double_test_labels])\n",
    "\n",
    "\n",
    "# Concatenate the data:\n",
    "test = np.concatenate((single_test, double_test))\n",
    "test_labels = np.concatenate((single_test_labels, double_test_labels))\n",
    "\n",
    "# Reuse the function from before to find the two highest energy pixels:\n",
    "test_pred = np.zeros((test.shape[0], 4))\n",
    "for i in range(test.shape[0]):\n",
    "    test_pred[i] = transform_label(test[i], single = False)\n",
    "\n",
    "# Evaluate the model using MSE and MAE:\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"Test RMSE: \", mean_squared_error(test_labels, test_pred)**0.5)\n",
    "print(\"Test MAE: \", np.mean(np.abs(test_labels - test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### TRAINING CODE ######\n",
    "# # Load modules\n",
    "# print(\"Loading modules...\")\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# # Prepare the data\n",
    "# input_shape = (512, 512, 1)\n",
    "\n",
    "# print(\"Reading in data...\")\n",
    "# # Read in data\n",
    "# x_train_singles = np.load(\"/mnt/home/taftkyle/indiv_data/singles/raw_single_train.npy\")\n",
    "# x_train_double = np.load(\"/mnt/home/taftkyle/indiv_data/doubles/raw_double_train.npy\")\n",
    "\n",
    "# # Combine the data\n",
    "# x_train = np.concatenate((x_train_singles, x_train_double))\n",
    "# y_train = np.load(\"y_train.npy\")\n",
    "\n",
    "# print(\"x_train shape:\", x_train.shape)\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# #Shuffle data\n",
    "# indices = np.arange(x_train.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "# x_train = x_train[indices]\n",
    "# y_train = y_train[indices]\n",
    "\n",
    "# # Define the model\n",
    "# model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "# model.add(Dropout(0.4))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(4, activation='linear'))  # (x,y) coordinates\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(\n",
    "#     optimizer=keras.optimizers.Adam(1e-3),\n",
    "#     loss=keras.losses.MeanSquaredError(),\n",
    "#     metrics=[\n",
    "#         keras.metrics.MeanSquaredError(name=\"mse\"),\n",
    "#         keras.metrics.MeanAbsoluteError(name=\"mae\"),\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# #Train model\n",
    "# print(\"Training model...\")\n",
    "\n",
    "# checkpoint_filepath = \"cnn_best_model_CMSE492\"\n",
    "# checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "#     checkpoint_filepath,\n",
    "#     monitor=\"val_mae\",\n",
    "#     mode = 'min',\n",
    "#     save_best_only=True,\n",
    "# )\n",
    "\n",
    "# history = model.fit(\n",
    "#         x=x_train,\n",
    "#         y=y_train,\n",
    "#         batch_size=32,\n",
    "#         epochs=100,\n",
    "#         validation_split=0.1,\n",
    "#         callbacks=[checkpoint_callback],\n",
    "# )\n",
    "\n",
    "\n",
    "# print(\"Model trained.\")\n",
    "\n",
    "# #Save model\n",
    "# model.save('cnn_model_CMSE492')\n",
    "\n",
    "# #Save history\n",
    "# np.save(\"cnn_history_CMSE492.npy\", history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [3018419.5, 1689657.75],\n",
       " 'mse': [3018419.5, 1689657.75],\n",
       " 'mae': [1216.6300048828125, 919.8373413085938],\n",
       " 'val_loss': [852182.25, 1106993.75],\n",
       " 'val_mse': [852182.25, 1106993.75],\n",
       " 'val_mae': [700.7245483398438, 781.4419555664062]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### EVALUATION CODE ######\n",
    "import numpy as np\n",
    "np.load(\"cnn_history_CMSE492.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### TRAINING CODE ######\n",
    "# # Load modules\n",
    "# print(\"Loading modules...\")\n",
    "\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# import tensorflow_addons as tfa\n",
    "\n",
    "# # Prepare the data\n",
    "\n",
    "# input_shape = (512, 512, 1)\n",
    "\n",
    "# print(\"Reading in data...\")\n",
    "# # Read in data\n",
    "# x_train_singles = np.load(\"/mnt/home/taftkyle/indiv_data/singles/raw_single_train.npy\")\n",
    "# x_train_double = np.load(\"/mnt/home/taftkyle/indiv_data/doubles/raw_double_train.npy\")\n",
    "\n",
    "# #Combine the data\n",
    "# x_train = np.concatenate((x_train_singles, x_train_double))\n",
    "# y_train = np.load(\"y_train.npy\")\n",
    "\n",
    "# print(\"x_train shape:\", x_train.shape)\n",
    "# print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# #Shuffle data\n",
    "# indices = np.arange(x_train.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "# x_train = x_train[indices]\n",
    "# y_train = y_train[indices]\n",
    "\n",
    "# learning_rate = 0.001\n",
    "# weight_decay = 0.0001\n",
    "# batch_size = 128\n",
    "# num_epochs = 100\n",
    "# image_size = 512  # We'll resize input images to this size\n",
    "# patch_size = 32  # Size of the patches to be extract from the input images\n",
    "# num_patches = (image_size // patch_size) ** 2\n",
    "# projection_dim = 64\n",
    "# num_heads = 4\n",
    "# transformer_units = [\n",
    "#     projection_dim * 2,\n",
    "#     projection_dim,\n",
    "# ]  # Size of the transformer layers\n",
    "# transformer_layers = 8\n",
    "# mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n",
    "\n",
    "\n",
    "# # Use data augmentation\n",
    "# data_augmentation = keras.Sequential(\n",
    "#     [\n",
    "#         layers.Normalization(),\n",
    "#         layers.Resizing(image_size, image_size),\n",
    "#     ],\n",
    "#     name=\"data_augmentation\",\n",
    "# )\n",
    "# # Compute the mean and the variance of the training data for normalization.\n",
    "# data_augmentation.layers[0].adapt(x_train)\n",
    "\n",
    "\n",
    "# # Implement multilayer perceptron (MLP)\n",
    "# def mlp(x, hidden_units, dropout_rate):\n",
    "#     for units in hidden_units:\n",
    "#         x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "#         x = layers.Dropout(dropout_rate)(x)\n",
    "#     return x\n",
    "\n",
    "\n",
    "# # Implement patch creation as a layer\n",
    "# class Patches(layers.Layer):\n",
    "#     def __init__(self, patch_size):\n",
    "#         super().__init__()\n",
    "#         self.patch_size = patch_size\n",
    "\n",
    "#     def call(self, images):\n",
    "#         batch_size = tf.shape(images)[0]\n",
    "#         patches = tf.image.extract_patches(\n",
    "#             images=images,\n",
    "#             sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "#             strides=[1, self.patch_size, self.patch_size, 1],\n",
    "#             rates=[1, 1, 1, 1],\n",
    "#             padding=\"VALID\",\n",
    "#         )\n",
    "#         patch_dims = patches.shape[-1]\n",
    "#         patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "#         return patches\n",
    "\n",
    "\n",
    "# # Display patches for a sample image\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(4, 4))\n",
    "# image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "# plt.imshow(image.astype(\"uint8\"))\n",
    "# plt.axis(\"off\")\n",
    "\n",
    "# resized_image = tf.image.resize(\n",
    "#     tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    "# )\n",
    "# print(\"Patch size:\", patch_size)\n",
    "# print(\"Image size:\", image_size)\n",
    "# print(\"Patches per image:\", num_patches)\n",
    "# print(\"Resized\", resized_image.shape)\n",
    "# patches = Patches(patch_size)(resized_image)\n",
    "# print(f\"Image size: {image_size} X {image_size}\")\n",
    "# print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "# print(f\"Patches per image: {patches.shape[1]}\")\n",
    "# print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "\n",
    "\n",
    "# class PatchEncoder(layers.Layer):\n",
    "#     def __init__(self, num_patches, projection_dim):\n",
    "#         super().__init__()\n",
    "#         self.num_patches = num_patches\n",
    "#         self.projection = layers.Dense(units=projection_dim)\n",
    "#         self.position_embedding = layers.Embedding(\n",
    "#             input_dim=num_patches, output_dim=projection_dim\n",
    "#         )\n",
    "\n",
    "#     def call(self, patch):\n",
    "#         positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "#         encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "#         return encoded\n",
    "\n",
    "\n",
    "\n",
    "# def create_vit_classifier():\n",
    "#     print(input_shape)\n",
    "#     inputs = layers.Input(shape=input_shape)\n",
    "#     # Augment data.\n",
    "#     augmented = data_augmentation(inputs)\n",
    "#     # Create patches.\n",
    "#     patches = Patches(patch_size)(augmented)\n",
    "#     # Encode patches.\n",
    "#     encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "#     # Create multiple layers of the Transformer block.\n",
    "#     for _ in range(transformer_layers):\n",
    "#         # Layer normalization 1.\n",
    "#         x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "#         # Create a multi-head attention layer.\n",
    "#         attention_output = layers.MultiHeadAttention(\n",
    "#             num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "#         )(x1, x1)\n",
    "#         # Skip connection 1.\n",
    "#         x2 = layers.Add()([attention_output, encoded_patches])\n",
    "#         # Layer normalization 2.\n",
    "#         x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "#         # MLP.\n",
    "#         x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "#         # Skip connection 2.\n",
    "#         encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "#     # Create a [batch_size, projection_dim] tensor.\n",
    "#     representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "#     representation = layers.Flatten()(representation)\n",
    "#     representation = layers.Dropout(0.5)(representation)\n",
    "\n",
    "#     # Add MLP.\n",
    "#     features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "\n",
    "#     # Output layers for peak localization and intensity prediction\n",
    "#     peak_localization = layers.Dense(4, activation='linear')(features)  # (x, y) coordinates of peak centers\n",
    "\n",
    "#     # Create the custom ViT model\n",
    "#     model = keras.Model(inputs=inputs, outputs=peak_localization)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Compile, train, and evaluate the mode\n",
    "\n",
    "# def run_experiment(model):\n",
    "#     optimizer = tfa.optimizers.AdamW(\n",
    "#         learning_rate=learning_rate, weight_decay=weight_decay\n",
    "#     )\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=optimizer,\n",
    "#         loss=keras.losses.MeanSquaredError(),\n",
    "#         metrics=[\n",
    "#             keras.metrics.MeanSquaredError(name=\"mse\"),\n",
    "#             keras.metrics.MeanAbsoluteError(name=\"mae\"),\n",
    "#         ],\n",
    "#     )\n",
    "\n",
    "#     checkpoint_filepath = \"tmp/checkpoint/vit_checkpoint\"\n",
    "#     checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "#         checkpoint_filepath,\n",
    "#         monitor=\"val_mae\",\n",
    "#         save_best_only=True,\n",
    "#         save_weights_only=True,\n",
    "#     )\n",
    "\n",
    "#     history = model.fit(\n",
    "#         x=x_train,\n",
    "#         y=y_train,\n",
    "#         batch_size=batch_size,\n",
    "#         epochs=num_epochs,\n",
    "#         validation_split=0.1,\n",
    "#         callbacks=[checkpoint_callback],\n",
    "#     )\n",
    "\n",
    "#     model.save(\"vit_model_CMSE492\")\n",
    "#     return history\n",
    "\n",
    "# vit_classifier = create_vit_classifier()\n",
    "# history = run_experiment(vit_classifier)\n",
    "\n",
    "# np.save(\"vit_history_CMSE492.npy\", history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [9161.853515625, 1063.215576171875],\n",
       " 'mse': [9161.853515625, 1063.215576171875],\n",
       " 'mae': [64.81576538085938, 24.205732345581055],\n",
       " 'val_loss': [663.7801513671875, 388.2276916503906],\n",
       " 'val_mse': [663.7801513671875, 388.2276916503906],\n",
       " 'val_mae': [17.322500228881836, 14.838343620300293]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### EVALUATION CODE ######\n",
    "import numpy as np\n",
    "np.load(\"history.npy\", allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(How do your models compare? What did you find when you carried out your methods? Some of your code related to\n",
    "presenting results/figures/data may be replicated from the methods section or may only be present in\n",
    "this section. All of the plots that you plan on using for your presentation should be present in this\n",
    "section)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion and Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(What did you learn from your results? What obstacles did you run into? What would you do differently next time? Clearly provide quantitative answers to your question(s)?  At least one of your questions should be answered with numbers.  That is, it is not sufficient to answer \"yes\" or \"no\", but rather to say something quantitative such as variable 1 increased roughly 10% for every 1 year increase in variable 2.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(List the source(s) for any data and/or literature cited in your project.  Ideally, this should be formatted using a formal citation format (MLA or APA or other, your choice!).   Multiple free online citation generators are available such as <a href=\"http://www.easybib.com/style\">http://www.easybib.com/style</a>. **Important:** if you use **any** code that you find on the internet for your project you **must** cite it or you risk losing most/all of the points for you project.)_\n",
    "\n",
    "\n",
    "Cade's code\n",
    "\n",
    "ViT code\n",
    "\n",
    "Image at top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
